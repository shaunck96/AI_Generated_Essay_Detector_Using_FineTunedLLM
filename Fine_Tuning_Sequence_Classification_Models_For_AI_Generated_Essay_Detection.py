{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNmNZrG/pKvRuLcfoRiMsq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunck96/AI_Generated_Essay_Detector_Using_FineTunedLLM/blob/main/Fine_Tuning_Sequence_Classification_Models_For_AI_Generated_Essay_Detection.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNzkT2jqiWnH",
        "outputId": "23459274-71ba-4db1-a7a6-8866ea52a385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "\u001b[31mERROR: Invalid requirement: 'transformers#==4.30'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->peft) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting bitsandbytes\n",
            "  Using cached https://test-files.pythonhosted.org/packages/5c/e0/597d593ec3b6cf5ea7eb4894a545045bd95611de8a316a2a1eaa838a2459/bitsandbytes-0.39.0-py3-none-any.whl (95.8 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.39.0\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.39.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2023.7.22)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "Collecting optuna\n",
            "  Using cached optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install xlrd\n",
        "!pip install -q peft --no-index --find-links /kaggle/input/llm-detect-pip/peft-0.5.0-py3-none-any.whl\n",
        "!!pip install -q language-tool-python --no-index --find-links /kaggle/input/daigt-misc/language_tool_python-2.7.1-py3-none-any.whl\n",
        "!!mkdir -p /root/.cache/language_tool_python/\n",
        "!!cp -r /content/input/daigt-misc/lang57/LanguageTool-5.7 /root/.cache/language_tool_python/LanguageTool-5.7\n",
        "!pip install transformers#==4.30\n",
        "!pip install peft\n",
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install language_tool_python\n",
        "!pip install optuna\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import time, sys, gc, logging, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import transformers\n",
        "import peft\n",
        "from accelerate import Accelerator\n",
        "import bitsandbytes\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from shutil import rmtree\n",
        "import language_tool_python\n",
        "import optuna\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import wait\n",
        "from transformers import BertForSequenceClassification, AutoTokenizer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer\n",
        "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import DebertaForSequenceClassification, DebertaTokenizer\n",
        "from transformers import MobileBertForSequenceClassification, MobileBertTokenizer\n",
        "from transformers import BartForSequenceClassification, BartTokenizer\n",
        "from transformers import ConvBertForSequenceClassification, ConvBertTokenizer\n",
        "from transformers import FunnelForSequenceClassification, FunnelTokenizer\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import torch\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import wait\n",
        "from scipy.special import expit as sigmoid"
      ],
      "metadata": {
        "id": "s5116AEdPTNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class load_training_data():\n",
        "\n",
        "  def __init__(self):\n",
        "    language_tool = language_tool_python.LanguageTool('en-US')\n",
        "    N_FOLD = 5\n",
        "    SEED = 42\n",
        "    DEBUG = True\n",
        "    IS_TRAIN = False\n",
        "\n",
        "    self.train_data = pd.DataFrame()\n",
        "\n",
        "    # Seed the same seed to all\n",
        "    def seed_everything(seed=42):\n",
        "        \"\"\"\n",
        "        Seeds the random number generators of Python's `random`, NumPy, and PyTorch to ensure reproducibility.\n",
        "\n",
        "        Parameters:\n",
        "        - seed (int): A seed value to be used for all random number generators.\n",
        "\n",
        "        Returns:\n",
        "        - None: This function does not return anything but sets the seed for various libraries.\n",
        "        \"\"\"\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    seed_everything()\n",
        "    # Create new `pandas` methods which use `tqdm` progress\n",
        "    # (can use tqdm_gui, optional kwargs, etc.)\n",
        "    tqdm.pandas()\n",
        "\n",
        "    log_level = \"DEBUG\"\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "        level=logging.WARNING\n",
        "    )\n",
        "\n",
        "    # set the main code and the modules it uses to the same log-level according to the node\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "\n",
        "    # Cross validation\n",
        "    def cv_split(self, train_data):\n",
        "        \"\"\"\n",
        "        Performs stratified K-fold cross-validation splitting on the training dataset.\n",
        "\n",
        "        Parameters:\n",
        "        - train_data (DataFrame): A pandas DataFrame containing the training data.\n",
        "\n",
        "        Returns:\n",
        "        - DataFrame: The input DataFrame with an additional column 'fold' indicating the fold assignment for each row.\n",
        "        \"\"\"\n",
        "        skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
        "        X = train_data.loc[:, train_data.columns != \"label\"]\n",
        "        y = train_data.loc[:, train_data.columns == \"label\"]\n",
        "\n",
        "        for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
        "            train_data.loc[valid_index, \"fold\"] = fold\n",
        "\n",
        "        print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
        "        display(train_data.head())\n",
        "        return train_data\n",
        "\n",
        "    def pre_processing_text(self, text):\n",
        "        \"\"\"\n",
        "        Processes and corrects typos in the given text using `language_tool_python`.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The text string to be processed.\n",
        "\n",
        "        Returns:\n",
        "        - str: The processed text with corrections applied.\n",
        "        \"\"\"\n",
        "        text = text.replace('\\n', ' ')\n",
        "        typos = language_tool.check(text) # typo is a list\n",
        "        # Check how many typos\n",
        "        #if len(typos) > 0:\n",
        "        #print(f\"The number of typos = {len(typos)}\\n {typos}\")\n",
        "        text = language_tool.correct(text)\n",
        "        return text\n",
        "\n",
        "    # Run pre-processing texts in parallel\n",
        "    def parallel_pre_processing_text(self, texts):\n",
        "        \"\"\"\n",
        "        Processes a list of texts in parallel, applying typo correction to each text.\n",
        "\n",
        "        Parameters:\n",
        "        - texts (list of str): A list of text strings to be processed.\n",
        "\n",
        "        Returns:\n",
        "        - list of str: A list of processed texts with corrections applied.\n",
        "        \"\"\"\n",
        "        print(f\"Total number of texts {len(texts)}\")\n",
        "        results = []\n",
        "        # run 'pre_processing' fucntions in the process pool\n",
        "        with ThreadPoolExecutor(4) as executor:\n",
        "            # results = list(tqdm(executor.map(pre_processing_text, texts)))\n",
        "            # send in the tasks\n",
        "            futures = [executor.submit(pre_processing_text, text) for text in texts]\n",
        "            # wait for all tasks to complete\n",
        "            for future in futures:\n",
        "                results.append(future.result())\n",
        "                if len(results) % 100 == 0:\n",
        "                    print(f\"Finished {len(results)} / {len(texts)}\\n\", end='', flush=True)\n",
        "        # wait for all tasks to complete\n",
        "        print(\"results\", len(results))\n",
        "        return results\n",
        "\n",
        "\n",
        "    def load_train_data():\n",
        "        \"\"\"\n",
        "        Loads and preprocesses the training data from specified CSV files.\n",
        "\n",
        "        Returns:\n",
        "        - DataFrame: A pandas DataFrame containing the combined and processed training data.\n",
        "        \"\"\"\n",
        "        train_df = pd.read_csv(\"/content/ai_generated_train_essays_gpt-4.csv\")\n",
        "        train_prompts_df = pd.read_csv(\"/content/train_prompts.csv\", sep=',')\n",
        "\n",
        "        # rename column generated to label and remove used 'id' and 'prompt_id' columns\n",
        "        # Label: 1 indicates generated texts (by LLMs)\n",
        "        train_df = train_df.rename(columns={'generated': 'label'})\n",
        "        train_df = train_df.reset_index(drop=True)\n",
        "        train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n",
        "    #     print(\"Start processing training data's text\")\n",
        "    #     start = time.time()\n",
        "    #     # Clear text in both train and test dataset\n",
        "    #     train_df['text'] = train_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
        "    #     display(train_df.head())\n",
        "    #     print(f\"Correct the training data's texts with {time.time() - start : .1f} seconds\")\n",
        "\n",
        "        # Include external data\n",
        "        external_df = pd.read_csv(\"/content/train_v2_drcat_02.csv\", sep=',')\n",
        "        # We only need 'text' and 'label' columns\n",
        "        external_df = external_df[[\"text\", \"label\"]]\n",
        "        external_df[\"label\"] = 1\n",
        "\n",
        "        xls_file_path = '/content/training_set_rel3.xls'\n",
        "        external_df_two = pd.read_excel(xls_file_path)\n",
        "        external_df_two = external_df_two[['essay']]\n",
        "        external_df_two.rename(columns={'essay':'text'},inplace=True)\n",
        "        external_df_two[\"label\"] = 0\n",
        "\n",
        "        external_df = pd.concat([external_df,external_df_two], axis=0)\n",
        "        print(\"Start processing external data's texts\")\n",
        "        start = time.time()\n",
        "        external_df['text'] = parallel_pre_processing_text(external_df['text'].to_list())\n",
        "        print(f\"Correct the external data's texts with {time.time() - start : .1f} seconds\")\n",
        "        #external_df['text'] = external_df['text'].map(lambda text: pre_processing_text(text))\n",
        "        display(external_df.head())\n",
        "        external_df.to_csv('train_v2_drcat_02_fixed.csv', index=False)\n",
        "        # Merge train and external data into train_data\n",
        "        train_data = pd.concat([train_df, external_df, external_df_two])\n",
        "        train_data.reset_index(inplace=True, drop=True)\n",
        "        # print(f\"Train data has shape: {train_data.shape}\")\n",
        "        print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
        "        return train_data\n",
        "\n",
        "    def training_data_setting_trigger(self):\n",
        "      self.train_data = self.load_train_data()\n",
        "      # Cross validation with 5 fold\n",
        "      self.train_data = self.cv_split(self.train_data)\n",
        "      # Train the model\n",
        "      fold = 0\n",
        "      return self.train_data"
      ],
      "metadata": {
        "id": "Tdm03PsAN3yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_trainer():\n",
        "\n",
        "  def __init__(self, model_name='gpt2', DEBUG=True)\n",
        "  # Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n",
        "  self.model_name = model_name\n",
        "  self.peft_config = LoraConfig(\n",
        "      r=64,\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.1,\n",
        "      bias=\"none\",\n",
        "      task_type=TaskType.SEQ_CLS,\n",
        "      inference_mode=False,\n",
        "      target_modules=[\n",
        "          \"q_proj\",\n",
        "          \"v_proj\"\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  self.bnb_config = BitsAndBytesConfig(\n",
        "      load_in_8bit=True,  # Enable 8-bit quantization\n",
        "      load_in_8bit_fp32_cpu_offload=True,  # Enable CPU offloading for certain layers\n",
        "      bnb_8bit_quant_type=\"nf8\",  # Type of 8-bit quantization, 'nf8' is one of the options\n",
        "      bnb_8bit_use_double_quant=True,  # Use double quantization\n",
        "      bnb_8bit_compute_dtype=torch.bfloat16,  # Data type for computation in 8-bit mode\n",
        "      bnb_8bit_blocksparse_layout=None,  # Block-sparse layout, use None for dense models\n",
        "      bnb_8bit_custom_kernel=False,  # Use custom kernel, false by default\n",
        "      bnb_8bit_cpu_offload=True,  # Enable CPU offloading\n",
        "      bnb_8bit_cpu_offload_dtype=torch.float32,  # Data type for CPU offloaded tensors\n",
        "      bnb_8bit_cpu_offload_use_pin_memory=True,  # Use pinned memory for CPU offloading\n",
        "      bnb_8bit_cpu_offload_use_fast_fp32_to_fp16_conversion=False  # Use fast conversion from FP32 to FP16\n",
        "  )\n",
        "\n",
        "  TARGET_MODEL = \"facebook/bart-large\"\n",
        "  peft_config = self.peft_config\n",
        "  bnb_config = self.bnb_config\n",
        "\n",
        "  self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "  base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      TARGET_MODEL,\n",
        "      num_labels=2,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  self.model = self.base_model\n",
        "  self.DEBUG = True\n",
        "\n",
        "  def load_model_mistral(self,fold):\n",
        "      \"\"\"\n",
        "      Loads the LLAMA model for a specific fold with the PEFT (Parameter-Efficient Fine-Tuning) configuration.\n",
        "\n",
        "      Parameters:\n",
        "      - fold (int): The fold number for which the model is to be loaded.\n",
        "\n",
        "      Returns:\n",
        "      - tuple: A tuple containing the loaded model and tokenizer.\n",
        "      \"\"\"\n",
        "      TARGET_MODEL = \"openlm-research/open_llama_3b\"\n",
        "\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n",
        "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "      self.model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n",
        "                                                                  num_labels=2, # label is 0 or 1\n",
        "                                                                  quantization_config=bnb_config,\n",
        "                                                                  device_map=\"auto\")\n",
        "      self.model.config.pretraining_tp = 1\n",
        "      self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/mistral-7b-v0-for-llm-detecting-competition/mistral_7b_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))\n",
        "\n",
        "      self.model.print_trainable_parameters()\n",
        "\n",
        "  def load_gpt3_model(self, fold):\n",
        "      TARGET_MODEL = \"EleutherAI/gpt3-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/gpt3_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_bart_large_model(self, fold):\n",
        "      TARGET_MODEL = \"facebook/bart-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/bart_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_t5_large_model(self, fold):\n",
        "      TARGET_MODEL = \"t5-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/t5_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_roberta_large_model(self, fold):\n",
        "      TARGET_MODEL = \"roberta-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.tokenizer.pad_token = tokenizer.eos_token\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/roberta_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_deberta_v3_large_model(self, fold):\n",
        "      TARGET_MODEL = \"microsoft/deberta-v3-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "      self.model = DebertaV2ForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\")\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/deberta_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_model_bert_cpu(self, fold):\n",
        "      \"\"\"\n",
        "      Loads the BERT model for a specific fold.\n",
        "\n",
        "      Parameters:\n",
        "      - fold (int): The fold number for which the model is to be loaded.\n",
        "\n",
        "      Returns:\n",
        "      - tuple: A tuple containing the loaded BERT model and tokenizer.\n",
        "      \"\"\"\n",
        "      TARGET_MODEL = \"bert-base-uncased\"\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=True)\n",
        "      self.model = BertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_roberta_model(self, fold):\n",
        "      TARGET_MODEL = \"roberta-base\"\n",
        "      self.tokenizer = RobertaTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_distilbert_model(self, fold):\n",
        "      TARGET_MODEL = \"distilbert-base-uncased\"\n",
        "      self.tokenizer = DistilBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = DistilBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_albert_model(self, fold):\n",
        "      TARGET_MODEL = \"albert-base-v2\"\n",
        "      self.tokenizer = AlbertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = AlbertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_gpt2_model(self, fold):\n",
        "      TARGET_MODEL = \"gpt2\"\n",
        "      self.tokenizer = GPT2Tokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = GPT2ForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "  def load_xlnet_model(self, fold):\n",
        "      TARGET_MODEL = \"xlnet-base-cased\"\n",
        "      self.tokenizer = XLNetTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = XLNetForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "  def load_electra_model(self, fold):\n",
        "      TARGET_MODEL = \"google/electra-small-discriminator\"\n",
        "      self.tokenizer = ElectraTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = ElectraForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_t5_model(self, fold):\n",
        "      TARGET_MODEL = \"t5-small\"\n",
        "      self.tokenizer = T5Tokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "          TARGET_MODEL\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_deberta_model(self, fold):\n",
        "      TARGET_MODEL = \"microsoft/deberta-base\"\n",
        "      self.tokenizer = DebertaTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = DebertaForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_mobilebert_model(self, fold):\n",
        "      TARGET_MODEL = \"google/mobilebert-uncased\"\n",
        "      self.tokenizer = MobileBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = MobileBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_bart_model(self, fold):\n",
        "      TARGET_MODEL = \"facebook/bart-base\"\n",
        "      self.tokenizer = BartTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = BartForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_convbert_model(self, fold):\n",
        "      TARGET_MODEL = \"YituTech/conv-bert-base\"\n",
        "      self.tokenizer = ConvBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = ConvBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_funnel_model(self, fold):\n",
        "      TARGET_MODEL = \"funnel-transformer/small\"\n",
        "      self.tokenizer = FunnelTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = FunnelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = model.cuda()\n",
        "\n",
        "\n",
        "  def preprocess_function(self, examples, tokenizer, max_length=512):\n",
        "      \"\"\"\n",
        "      Tokenizes and processes the text data using the provided tokenizer.\n",
        "\n",
        "      Parameters:\n",
        "      - examples (dict): A dictionary containing the text data.\n",
        "      - tokenizer: The tokenizer to be used for processing.\n",
        "      - max_length (int): The maximum length of the tokenized sequences.\n",
        "\n",
        "      Returns:\n",
        "      - dict: A dictionary containing the processed and tokenized text data.\n",
        "      \"\"\"\n",
        "      examples[\"text\"] = list(map(lambda text: pre_processing_text(text), examples[\"text\"]))\n",
        "      return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n",
        "\n",
        "  def compute_metrics(self, eval_pred):\n",
        "      \"\"\"\n",
        "      Computes evaluation metrics for the model predictions.\n",
        "\n",
        "      Parameters:\n",
        "      - eval_pred (tuple): A tuple containing model predictions and actual labels.\n",
        "\n",
        "      Returns:\n",
        "      - dict: A dictionary containing computed metrics like accuracy and ROC-AUC.\n",
        "      \"\"\"\n",
        "      predictions, labels = eval_pred\n",
        "      predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "      accuracy_val = accuracy_score(labels, predictions)\n",
        "      roc_auc_val = roc_auc_score(labels, predictions)\n",
        "      r = { \"accuracy\": accuracy_val,\n",
        "            \"roc_auc\": roc_auc_val}\n",
        "      # logging.debug(f'{r}')\n",
        "      return r\n",
        "\n",
        "\n",
        "  def train_model_by_fold(self, fold, model, tokenizer):\n",
        "        \"\"\"\n",
        "        Trains the model on a specified fold of the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        - fold (int): The fold number to train the model on.\n",
        "\n",
        "        Returns:\n",
        "        - None: This function does not return anything but trains the model on the specified fold.\n",
        "        \"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f\"Start training the fold {fold} model\")\n",
        "        # Create train and valid dataset for a fold\n",
        "        fold_valid_df = train_data[train_data[\"fold\"] == fold]\n",
        "        fold_train_df = train_data[train_data[\"fold\"] != fold]\n",
        "        # Train the model with small (for debugging) or large samples\n",
        "        if DEBUG:\n",
        "            fold_train_df = fold_train_df.sample(frac =.05, random_state=SEED)\n",
        "            fold_valid_df = fold_valid_df.sample(frac =.05, random_state=SEED)\n",
        "        else:\n",
        "            fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n",
        "            fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n",
        "\n",
        "        print(f'fold_train_df {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
        "        print(f'fold_valid_df {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
        "        # create the dataset\n",
        "        train_ds = Dataset.from_pandas(fold_train_df)\n",
        "        valid_ds = Dataset.from_pandas(fold_valid_df)\n",
        "\n",
        "        # Tokenize the train and valid dataset and pass tokenizer as function argument\n",
        "        train_tokenized_ds = train_ds.map(self.preprocess_function, batched=True,\n",
        "                                          fn_kwargs={\"tokenizer\": tokenizer})\n",
        "        valid_tokenized_ds = valid_ds.map(self.preprocess_function, batched=True,\n",
        "                                          fn_kwargs={\"tokenizer\": tokenizer})\n",
        "        # Create data collator with padding (padding to the longest sequence)\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
        "\n",
        "        # Start training processing\n",
        "        TMP_DIR = Path(f\"/content/tmp/{model_name}{fold}/\")\n",
        "        TMP_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        STEPS = 5 if self.DEBUG else 20\n",
        "        EPOCHS = 1 if self.DEBUG else 10\n",
        "        BATCH_SIZE = 2\n",
        "        training_args = TrainingArguments(output_dir=TMP_DIR,\n",
        "                                          learning_rate=5e-5,\n",
        "                                          per_device_train_batch_size=BATCH_SIZE,\n",
        "                                          per_device_eval_batch_size=1,\n",
        "                                          gradient_accumulation_steps=16,\n",
        "                                          max_grad_norm=0.3,\n",
        "                                          optim='paged_adamw_32bit',\n",
        "                                          lr_scheduler_type=\"cosine\",\n",
        "                                          num_train_epochs=EPOCHS,\n",
        "                                          weight_decay=0.01,\n",
        "                                          evaluation_strategy=\"epoch\",\n",
        "                                          save_strategy=\"epoch\",\n",
        "                                          load_best_model_at_end=True,\n",
        "                                          push_to_hub=False,\n",
        "                                          warmup_steps=STEPS,\n",
        "                                          eval_steps=STEPS,\n",
        "                                          logging_steps=STEPS,\n",
        "                                          report_to='none', # if DEBUG else 'wandb'\n",
        "                                          log_level='warning', # 'warning' is default level\n",
        "                                        )\n",
        "\n",
        "\n",
        "        # Create the trainer\n",
        "        trainer = Trainer(model=model,\n",
        "                          args=training_args,\n",
        "                          train_dataset=train_tokenized_ds,\n",
        "                          eval_dataset=valid_tokenized_ds,\n",
        "                          tokenizer=tokenizer,\n",
        "                          data_collator=data_collator,\n",
        "                          compute_metrics=compute_metrics)\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        OUTPUT_DIR = Path(f\"/content/working/{model_name}{fold}/\")\n",
        "        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "        trainer.save_model(output_dir=str(OUTPUT_DIR))\n",
        "        print(f\"=== Finish the training for fold {fold} ===\")\n",
        "        del model, trainer, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "  def language_model_training_trigger(self, fold):\n",
        "      \"\"\"\n",
        "      Triggers the training of the language model based on the IS_TRAIN flag.\n",
        "\n",
        "      Parameters:\n",
        "      - IS_TRAIN (bool): Flag indicating whether to train the model.\n",
        "\n",
        "      Returns:\n",
        "      - None: This function does not return anything but triggers the training process.\n",
        "      \"\"\"\n",
        "      start = time.time()\n",
        "      # Load train data\n",
        "      train_data = load_train_data()\n",
        "      # Cross validation with 5 fold\n",
        "      train_data = cv_split(train_data)\n",
        "      # Train the model\n",
        "      fold = fold\n",
        "      if self.model_name == 'llama with peft':\n",
        "          model,tokenizer = self.load_model_mistral(fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'gpt3':\n",
        "          model,tokenizer = self.load_gpt3_model(fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'bart large':\n",
        "          model,tokenizer = self.load_bart_large_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 't5 large':\n",
        "          model,tokenizer = self.load_t5_large_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'roberta large':\n",
        "          model,tokenizer = self.load_roberta_large_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'deberta v3 large':\n",
        "          model,tokenizer = self.load_deberta_v3_large_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'bert':\n",
        "          model,tokenizer = self.load_model_bert_cpu(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'roberta':\n",
        "          model,tokenizer = self.load_roberta_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'distilbert':\n",
        "          model,tokenizer = self.load_distilbert_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'albert':\n",
        "          model,tokenizer = self.load_albert_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'gpt2':\n",
        "          model,tokenizer = self.load_gpt2_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'xlnet':\n",
        "          model,tokenizer = self.load_xlnet_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'electra':\n",
        "          model,tokenizer = self.load_electra_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 't5 small':\n",
        "          model,tokenizer = self.load_t5_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'deberta':\n",
        "          model,tokenizer = self.load_deberta_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'mobilebert':\n",
        "          model,tokenizer = self.load_mobilebert_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'bart':\n",
        "          model,tokenizer = self.load_bart_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'convbert':\n",
        "          model,tokenizer = self.load_convbert_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      elif self.model_name == 'funnel':\n",
        "          model,tokenizer = self.load_funnel_model(self.fold)\n",
        "          self.train_model_by_fold(fold,model,tokenizer)\n",
        "      else:\n",
        "          raise ValueError(\"Model not recognized or not supported.\")\n",
        "    #     # Add multiple threads to run each fold model concurrently\n",
        "\n",
        "    #with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    #     futures = [executor.submit(train_model_by_fold, fold) for fold in range(2)]\n",
        "    #     # wait for all tasks to complete\n",
        "    #    wait(futures)\n",
        "    #    print('All training tasks are done!')\n",
        "\n",
        "    #for idx, fold in enumerate(range(N_FOLD)):\n",
        "    #sys.exit(f\"Training time of fold {fold} = {time.time() - start: .1f} seconds\")\n",
        "\n",
        "\n",
        "# Cross validation with 5 fold\n",
        "train_data = load_training_data.training_data_setting_trigger()\n",
        "# Train the model\n",
        "fold = 0\n",
        "model_trainer.(model_name='gpt2', DEBUG=True).language_model_training_trigger(fold)"
      ],
      "metadata": {
        "id": "uw7JExiMBmLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLfUGEZribqZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}