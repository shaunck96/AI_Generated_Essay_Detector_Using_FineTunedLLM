{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMkZQ0nx04I6RDnsaZLR2DZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "889c8318fa1d4a18bceec6c13563d141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edb87b75f3a8426b8d1b5a8b116547dd",
              "IPY_MODEL_8144987dbb8645ad97dc609e73e6c61a",
              "IPY_MODEL_8cb17dd7260e4ab4b3c44934ebe45d55"
            ],
            "layout": "IPY_MODEL_d3e5ab11b9e24dee964d3a2006090644"
          }
        },
        "edb87b75f3a8426b8d1b5a8b116547dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b2e2593d994925b902ab0c699ecc3b",
            "placeholder": "​",
            "style": "IPY_MODEL_56cd0d8913e54ccfa86d09aa3ec7ed7d",
            "value": "Map:   0%"
          }
        },
        "8144987dbb8645ad97dc609e73e6c61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0dd6e4247440808de58bc4d93dc5dc",
            "max": 2841,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b1645e37de64b8587f4f8ed37f380fa",
            "value": 0
          }
        },
        "8cb17dd7260e4ab4b3c44934ebe45d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76f9e97d9ea4aa78ee791bb4fd25b57",
            "placeholder": "​",
            "style": "IPY_MODEL_9fb750391c0e413f854622b3deb078fe",
            "value": " 0/2841 [00:00&lt;?, ? examples/s]"
          }
        },
        "d3e5ab11b9e24dee964d3a2006090644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b2e2593d994925b902ab0c699ecc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56cd0d8913e54ccfa86d09aa3ec7ed7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad0dd6e4247440808de58bc4d93dc5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1645e37de64b8587f4f8ed37f380fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d76f9e97d9ea4aa78ee791bb4fd25b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb750391c0e413f854622b3deb078fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunck96/AI_Generated_Essay_Detector_Using_FineTunedLLM/blob/main/Fine_Tuning_Sequence_Classification_Models_For_AI_Generated_Essay_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNzkT2jqiWnH",
        "outputId": "0e94fce4-fdf9-4c84-bc1e-7938b5656f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "\u001b[33mWARNING: Location '/kaggle/input/llm-detect-pip/peft-0.5.0-py3-none-any.whl' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement peft (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for peft\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Invalid requirement: 'transformers#==4.30'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting peft\n",
            "  Downloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->peft) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.24.1 peft-0.6.2\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting bitsandbytes\n",
            "  Downloading https://test-files.pythonhosted.org/packages/5c/e0/597d593ec3b6cf5ea7eb4894a545045bd95611de8a316a2a1eaa838a2459/bitsandbytes-0.39.0-py3-none-any.whl (95.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.39.0\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.39.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2023.7.22)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install xlrd\n",
        "!pip install -q peft --no-index --find-links /kaggle/input/llm-detect-pip/peft-0.5.0-py3-none-any.whl\n",
        "!!pip install -q language-tool-python --no-index --find-links /kaggle/input/daigt-misc/language_tool_python-2.7.1-py3-none-any.whl\n",
        "!!mkdir -p /root/.cache/language_tool_python/\n",
        "!!cp -r /content/input/daigt-misc/lang57/LanguageTool-5.7 /root/.cache/language_tool_python/LanguageTool-5.7\n",
        "!pip install transformers#==4.30\n",
        "!pip install peft\n",
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install language_tool_python\n",
        "!pip install optuna\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import time, sys, gc, logging, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import transformers\n",
        "import peft\n",
        "from accelerate import Accelerator\n",
        "import bitsandbytes\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from shutil import rmtree\n",
        "import language_tool_python\n",
        "import optuna\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import wait\n",
        "from transformers import BertForSequenceClassification, AutoTokenizer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer\n",
        "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import DebertaForSequenceClassification, DebertaTokenizer\n",
        "from transformers import MobileBertForSequenceClassification, MobileBertTokenizer\n",
        "from transformers import BartForSequenceClassification, BartTokenizer\n",
        "from transformers import ConvBertForSequenceClassification, ConvBertTokenizer\n",
        "from transformers import FunnelForSequenceClassification, FunnelTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import torch\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import wait\n",
        "from scipy.special import expit as sigmoid"
      ],
      "metadata": {
        "id": "s5116AEdPTNn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class load_training_data():\n",
        "\n",
        "  def __init__(self):\n",
        "    language_tool = language_tool_python.LanguageTool('en-US')\n",
        "    N_FOLD = 5\n",
        "    SEED = 42\n",
        "    DEBUG = True\n",
        "    IS_TRAIN = False\n",
        "\n",
        "    self.train_data = pd.DataFrame()\n",
        "\n",
        "    # Cross validation\n",
        "  def cv_split(self, train_data):\n",
        "      \"\"\"\n",
        "      Performs stratified K-fold cross-validation splitting on the training dataset.\n",
        "\n",
        "      Parameters:\n",
        "      - train_data (DataFrame): A pandas DataFrame containing the training data.\n",
        "\n",
        "      Returns:\n",
        "      - DataFrame: The input DataFrame with an additional column 'fold' indicating the fold assignment for each row.\n",
        "      \"\"\"\n",
        "      N_FOLD = 5\n",
        "      SEED = 42\n",
        "      skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
        "      X = train_data.loc[:, train_data.columns != \"label\"]\n",
        "      y = train_data.loc[:, train_data.columns == \"label\"]\n",
        "\n",
        "      for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
        "          train_data.loc[valid_index, \"fold\"] = fold\n",
        "\n",
        "      print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
        "      display(train_data.head())\n",
        "      return train_data\n",
        "\n",
        "  def pre_processing_text(self, text):\n",
        "      \"\"\"\n",
        "      Processes and corrects typos in the given text using `language_tool_python`.\n",
        "\n",
        "      Parameters:\n",
        "      - text (str): The text string to be processed.\n",
        "\n",
        "      Returns:\n",
        "      - str: The processed text with corrections applied.\n",
        "      \"\"\"\n",
        "      text = text.replace('\\n', ' ')\n",
        "      typos = language_tool.check(text) # typo is a list\n",
        "      # Check how many typos\n",
        "      #if len(typos) > 0:\n",
        "      #print(f\"The number of typos = {len(typos)}\\n {typos}\")\n",
        "      text = language_tool.correct(text)\n",
        "      return text\n",
        "\n",
        "  # Run pre-processing texts in parallel\n",
        "  def parallel_pre_processing_text(self, texts):\n",
        "      \"\"\"\n",
        "      Processes a list of texts in parallel, applying typo correction to each text.\n",
        "\n",
        "      Parameters:\n",
        "      - texts (list of str): A list of text strings to be processed.\n",
        "\n",
        "      Returns:\n",
        "      - list of str: A list of processed texts with corrections applied.\n",
        "      \"\"\"\n",
        "      print(f\"Total number of texts {len(texts)}\")\n",
        "      results = []\n",
        "      # run 'pre_processing' fucntions in the process pool\n",
        "      with ThreadPoolExecutor(4) as executor:\n",
        "          # results = list(tqdm(executor.map(pre_processing_text, texts)))\n",
        "          # send in the tasks\n",
        "          futures = [executor.submit(pre_processing_text, text) for text in texts]\n",
        "          # wait for all tasks to complete\n",
        "          for future in futures:\n",
        "              results.append(future.result())\n",
        "              if len(results) % 100 == 0:\n",
        "                  print(f\"Finished {len(results)} / {len(texts)}\\n\", end='', flush=True)\n",
        "      # wait for all tasks to complete\n",
        "      print(\"results\", len(results))\n",
        "      return results\n",
        "\n",
        "\n",
        "  def load_train_data(self):\n",
        "      \"\"\"\n",
        "      Loads and preprocesses the training data from specified CSV files.\n",
        "\n",
        "      Returns:\n",
        "      - DataFrame: A pandas DataFrame containing the combined and processed training data.\n",
        "      \"\"\"\n",
        "      train_df = pd.read_csv(\"/content/ai_generated_train_essays_gpt-4.csv\")\n",
        "      train_prompts_df = pd.read_csv(\"/content/train_prompts.csv\", sep=',')\n",
        "\n",
        "      # rename column generated to label and remove used 'id' and 'prompt_id' columns\n",
        "      # Label: 1 indicates generated texts (by LLMs)\n",
        "      train_df = train_df.rename(columns={'generated': 'label'})\n",
        "      train_df = train_df.reset_index(drop=True)\n",
        "      train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n",
        "  #     print(\"Start processing training data's text\")\n",
        "  #     start = time.time()\n",
        "  #     # Clear text in both train and test dataset\n",
        "  #     train_df['text'] = train_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
        "  #     display(train_df.head())\n",
        "  #     print(f\"Correct the training data's texts with {time.time() - start : .1f} seconds\")\n",
        "\n",
        "      # Include external data\n",
        "      external_df = pd.read_csv(\"/content/train_v2_drcat_02.csv\", sep=',')\n",
        "      # We only need 'text' and 'label' columns\n",
        "      external_df = external_df[[\"text\", \"label\"]]\n",
        "      external_df[\"label\"] = 1\n",
        "\n",
        "      xls_file_path = '/content/training_set_rel3.xls'\n",
        "      external_df_two = pd.read_excel(xls_file_path)\n",
        "      external_df_two = external_df_two[['essay']]\n",
        "      external_df_two.rename(columns={'essay':'text'},inplace=True)\n",
        "      external_df_two[\"label\"] = 0\n",
        "\n",
        "      external_df = pd.concat([external_df,external_df_two], axis=0)\n",
        "      print(\"Start processing external data's texts\")\n",
        "      #start = time.time()\n",
        "      #external_df['text'] = self.parallel_pre_processing_text(external_df['text'].to_list())\n",
        "      #print(f\"Correct the external data's texts with {time.time() - start : .1f} seconds\")\n",
        "      #external_df['text'] = external_df['text'].map(lambda text: self.pre_processing_text(text))\n",
        "      #display(external_df.head())\n",
        "      #external_df.to_csv('train_v2_drcat_02_fixed.csv', index=False)\n",
        "      # Merge train and external data into train_data\n",
        "      train_data = pd.concat([train_df, external_df, external_df_two])\n",
        "      train_data.reset_index(inplace=True, drop=True)\n",
        "      # print(f\"Train data has shape: {train_data.shape}\")\n",
        "      print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
        "      return train_data\n",
        "\n",
        "  def training_data_setting_trigger(self):\n",
        "    self.train_data = self.load_train_data()\n",
        "    # Cross validation with 5 fold\n",
        "    self.train_data = self.cv_split(self.train_data)\n",
        "    return self.train_data"
      ],
      "metadata": {
        "id": "Tdm03PsAN3yM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_trainer():\n",
        "\n",
        "  def __init__(self, model_name='gpt2', train_data = pd.DataFrame(), DEBUG=True):\n",
        "  # Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n",
        "    self.model_name = model_name\n",
        "    self.peft_config = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"v_proj\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    self.bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,  # Enable 8-bit quantization\n",
        "        load_in_8bit_fp32_cpu_offload=True,  # Enable CPU offloading for certain layers\n",
        "        bnb_8bit_quant_type=\"nf8\",  # Type of 8-bit quantization, 'nf8' is one of the options\n",
        "        bnb_8bit_use_double_quant=True,  # Use double quantization\n",
        "        bnb_8bit_compute_dtype=torch.bfloat16,  # Data type for computation in 8-bit mode\n",
        "        bnb_8bit_blocksparse_layout=None,  # Block-sparse layout, use None for dense models\n",
        "        bnb_8bit_custom_kernel=False,  # Use custom kernel, false by default\n",
        "        bnb_8bit_cpu_offload=True,  # Enable CPU offloading\n",
        "        bnb_8bit_cpu_offload_dtype=torch.float32,  # Data type for CPU offloaded tensors\n",
        "        bnb_8bit_cpu_offload_use_pin_memory=True,  # Use pinned memory for CPU offloading\n",
        "        bnb_8bit_cpu_offload_use_fast_fp32_to_fp16_conversion=False  # Use fast conversion from FP32 to FP16\n",
        "    )\n",
        "\n",
        "    TARGET_MODEL = \"facebook/bart-large\"\n",
        "    peft_config = self.peft_config\n",
        "    bnb_config = self.bnb_config\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        TARGET_MODEL,\n",
        "        num_labels=2,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    self.model = base_model\n",
        "    self.DEBUG = True\n",
        "    self.train_data = train_data\n",
        "\n",
        "  def load_model_mistral(self,fold):\n",
        "      \"\"\"\n",
        "      Loads the LLAMA model for a specific fold with the PEFT (Parameter-Efficient Fine-Tuning) configuration.\n",
        "\n",
        "      Parameters:\n",
        "      - fold (int): The fold number for which the model is to be loaded.\n",
        "\n",
        "      Returns:\n",
        "      - tuple: A tuple containing the loaded model and tokenizer.\n",
        "      \"\"\"\n",
        "      TARGET_MODEL = \"openlm-research/open_llama_3b\"\n",
        "\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n",
        "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "      self.model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n",
        "                                                                  num_labels=2, # label is 0 or 1\n",
        "                                                                  quantization_config=bnb_config,\n",
        "                                                                  device_map=\"auto\")\n",
        "      self.model.config.pretraining_tp = 1\n",
        "      self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/mistral-7b-v0-for-llm-detecting-competition/mistral_7b_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))\n",
        "\n",
        "      self.model.print_trainable_parameters()\n",
        "\n",
        "  def load_gpt3_model(self, fold):\n",
        "      TARGET_MODEL = \"EleutherAI/gpt3-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/gpt3_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_bart_large_model(self, fold):\n",
        "      TARGET_MODEL = \"facebook/bart-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/bart_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_t5_large_model(self, fold):\n",
        "      TARGET_MODEL = \"t5-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/t5_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_roberta_large_model(self, fold):\n",
        "      TARGET_MODEL = \"roberta-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.tokenizer.pad_token = tokenizer.eos_token\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/roberta_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_deberta_v3_large_model(self, fold):\n",
        "      TARGET_MODEL = \"microsoft/deberta-v3-large\"\n",
        "      peft_config = self.peft_config\n",
        "      bnb_config = self.bnb_config\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
        "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "      self.model = DebertaV2ForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2,\n",
        "          quantization_config=bnb_config,\n",
        "          device_map=\"auto\")\n",
        "\n",
        "      if IS_TRAIN:\n",
        "          self.model = get_peft_model(self.model, peft_config)\n",
        "      else:\n",
        "          OUTPUT_DIR = f\"/content/deberta_large_fold{fold}\"\n",
        "          self.model = PeftModel.from_pretrained(self.model, str(OUTPUT_DIR))\n",
        "\n",
        "\n",
        "  def load_model_bert_cpu(self, fold):\n",
        "      \"\"\"\n",
        "      Loads the BERT model for a specific fold.\n",
        "\n",
        "      Parameters:\n",
        "      - fold (int): The fold number for which the model is to be loaded.\n",
        "\n",
        "      Returns:\n",
        "      - tuple: A tuple containing the loaded BERT model and tokenizer.\n",
        "      \"\"\"\n",
        "      TARGET_MODEL = \"bert-base-uncased\"\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=True)\n",
        "      self.model = BertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_roberta_model(self, fold):\n",
        "      TARGET_MODEL = \"roberta-base\"\n",
        "      self.tokenizer = RobertaTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_distilbert_model(self, fold):\n",
        "      TARGET_MODEL = \"distilbert-base-uncased\"\n",
        "      self.tokenizer = DistilBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = DistilBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_albert_model(self, fold):\n",
        "      TARGET_MODEL = \"albert-base-v2\"\n",
        "      self.tokenizer = AlbertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = AlbertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_gpt2_model(self, fold):\n",
        "      TARGET_MODEL = \"gpt2\"\n",
        "      self.tokenizer = GPT2Tokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = GPT2ForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "  def load_xlnet_model(self, fold):\n",
        "      TARGET_MODEL = \"xlnet-base-cased\"\n",
        "      self.tokenizer = XLNetTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = XLNetForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "  def load_electra_model(self, fold):\n",
        "      TARGET_MODEL = \"google/electra-small-discriminator\"\n",
        "      self.tokenizer = ElectraTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = ElectraForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_t5_model(self, fold):\n",
        "      TARGET_MODEL = \"t5-small\"\n",
        "      self.tokenizer = T5Tokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "          TARGET_MODEL\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_deberta_model(self, fold):\n",
        "      TARGET_MODEL = \"microsoft/deberta-base\"\n",
        "      self.tokenizer = DebertaTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = DebertaForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_mobilebert_model(self, fold):\n",
        "      TARGET_MODEL = \"google/mobilebert-uncased\"\n",
        "      self.tokenizer = MobileBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = MobileBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_bart_model(self, fold):\n",
        "      TARGET_MODEL = \"facebook/bart-base\"\n",
        "      self.tokenizer = BartTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = BartForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_convbert_model(self, fold):\n",
        "      TARGET_MODEL = \"YituTech/conv-bert-base\"\n",
        "      self.tokenizer = ConvBertTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = ConvBertForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "\n",
        "\n",
        "  def load_funnel_model(self, fold):\n",
        "      TARGET_MODEL = \"funnel-transformer/small\"\n",
        "      self.tokenizer = FunnelTokenizer.from_pretrained(TARGET_MODEL)\n",
        "\n",
        "      self.model = FunnelForSequenceClassification.from_pretrained(\n",
        "          TARGET_MODEL,\n",
        "          num_labels=2\n",
        "      )\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          self.model = model.cuda()\n",
        "\n",
        "\n",
        "  def pre_processing_text(self, text):\n",
        "      \"\"\"\n",
        "      Processes and corrects typos in the given text using `language_tool_python`.\n",
        "\n",
        "      Parameters:\n",
        "      - text (str): The text string to be processed.\n",
        "\n",
        "      Returns:\n",
        "      - str: The processed text with corrections applied.\n",
        "      \"\"\"\n",
        "      language_tool = language_tool_python.LanguageTool('en-US')\n",
        "      text = text.replace('\\n', ' ')\n",
        "      typos = language_tool.check(text) # typo is a list\n",
        "      # Check how many typos\n",
        "      #if len(typos) > 0:\n",
        "      #print(f\"The number of typos = {len(typos)}\\n {typos}\")\n",
        "      text = language_tool.correct(text)\n",
        "      return text\n",
        "\n",
        "\n",
        "  def preprocess_function(self, examples, tokenizer):\n",
        "      \"\"\"\n",
        "      Tokenizes and processes the text data using the provided tokenizer.\n",
        "\n",
        "      Parameters:\n",
        "      - examples (dict): A dictionary containing the text data.\n",
        "      - tokenizer: The tokenizer to be used for processing.\n",
        "      - max_length (int): The maximum length of the tokenized sequences.\n",
        "\n",
        "      Returns:\n",
        "      - dict: A dictionary containing the processed and tokenized text data.\n",
        "      \"\"\"\n",
        "      max_length=512\n",
        "      examples[\"text\"] = list(map(lambda text: self.pre_processing_text(text), examples[\"text\"]))\n",
        "      return self.tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n",
        "\n",
        "  def compute_metrics(self, eval_pred):\n",
        "      \"\"\"\n",
        "      Computes evaluation metrics for the model predictions.\n",
        "\n",
        "      Parameters:\n",
        "      - eval_pred (tuple): A tuple containing model predictions and actual labels.\n",
        "\n",
        "      Returns:\n",
        "      - dict: A dictionary containing computed metrics like accuracy and ROC-AUC.\n",
        "      \"\"\"\n",
        "      predictions, labels = eval_pred\n",
        "      predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "      accuracy_val = accuracy_score(labels, predictions)\n",
        "      roc_auc_val = roc_auc_score(labels, predictions)\n",
        "      r = { \"accuracy\": accuracy_val,\n",
        "            \"roc_auc\": roc_auc_val}\n",
        "      # logging.debug(f'{r}')\n",
        "      return r\n",
        "\n",
        "\n",
        "  def train_model_by_fold(self, fold, model, tokenizer):\n",
        "        \"\"\"\n",
        "        Trains the model on a specified fold of the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        - fold (int): The fold number to train the model on.\n",
        "\n",
        "        Returns:\n",
        "        - None: This function does not return anything but trains the model on the specified fold.\n",
        "        \"\"\"\n",
        "        SEED = 42\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f\"Start training the fold {fold} model\")\n",
        "        # Create train and valid dataset for a fold\n",
        "        fold_valid_df = self.train_data[self.train_data[\"fold\"] == fold]\n",
        "        fold_train_df = self.train_data[self.train_data[\"fold\"] != fold]\n",
        "        # Train the model with small (for debugging) or large samples\n",
        "        if self.DEBUG:\n",
        "            fold_train_df = fold_train_df.sample(frac =.05, random_state=SEED)\n",
        "            fold_valid_df = fold_valid_df.sample(frac =.05, random_state=SEED)\n",
        "        else:\n",
        "            fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n",
        "            fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n",
        "\n",
        "        print(f'fold_train_df {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
        "        print(f'fold_valid_df {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
        "        # create the dataset\n",
        "        train_ds = Dataset.from_pandas(fold_train_df)\n",
        "        valid_ds = Dataset.from_pandas(fold_valid_df)\n",
        "\n",
        "        # Tokenize the train and valid dataset and pass tokenizer as function argument\n",
        "        train_tokenized_ds = train_ds.map(self.preprocess_function, batched=True,\n",
        "                                          fn_kwargs={\"tokenizer\": tokenizer})\n",
        "        valid_tokenized_ds = valid_ds.map(self.preprocess_function, batched=True,\n",
        "                                          fn_kwargs={\"tokenizer\": tokenizer})\n",
        "        # Create data collator with padding (padding to the longest sequence)\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
        "\n",
        "        # Start training processing\n",
        "        TMP_DIR = Path(f\"/content/tmp/{model_name}{fold}/\")\n",
        "        TMP_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        STEPS = 5 if self.DEBUG else 20\n",
        "        EPOCHS = 1 if self.DEBUG else 10\n",
        "        BATCH_SIZE = 2\n",
        "        training_args = TrainingArguments(output_dir=TMP_DIR,\n",
        "                                          learning_rate=5e-5,\n",
        "                                          per_device_train_batch_size=BATCH_SIZE,\n",
        "                                          per_device_eval_batch_size=1,\n",
        "                                          gradient_accumulation_steps=16,\n",
        "                                          max_grad_norm=0.3,\n",
        "                                          optim='paged_adamw_32bit',\n",
        "                                          lr_scheduler_type=\"cosine\",\n",
        "                                          num_train_epochs=EPOCHS,\n",
        "                                          weight_decay=0.01,\n",
        "                                          evaluation_strategy=\"epoch\",\n",
        "                                          save_strategy=\"epoch\",\n",
        "                                          load_best_model_at_end=True,\n",
        "                                          push_to_hub=False,\n",
        "                                          warmup_steps=STEPS,\n",
        "                                          eval_steps=STEPS,\n",
        "                                          logging_steps=STEPS,\n",
        "                                          report_to='none', # if DEBUG else 'wandb'\n",
        "                                          log_level='warning', # 'warning' is default level\n",
        "                                        )\n",
        "\n",
        "\n",
        "        # Create the trainer\n",
        "        trainer = Trainer(model=model,\n",
        "                          args=training_args,\n",
        "                          train_dataset=train_tokenized_ds,\n",
        "                          eval_dataset=valid_tokenized_ds,\n",
        "                          tokenizer=tokenizer,\n",
        "                          data_collator=data_collator,\n",
        "                          compute_metrics=compute_metrics)\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        OUTPUT_DIR = Path(f\"/content/working/{model_name}{fold}/\")\n",
        "        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "        trainer.save_model(output_dir=str(OUTPUT_DIR))\n",
        "        print(f\"=== Finish the training for fold {fold} ===\")\n",
        "        del model, trainer, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "  def language_model_training_trigger(self, fold):\n",
        "      \"\"\"\n",
        "      Triggers the training of the language model based on the IS_TRAIN flag.\n",
        "\n",
        "      Parameters:\n",
        "      - IS_TRAIN (bool): Flag indicating whether to train the model.\n",
        "\n",
        "      Returns:\n",
        "      - None: This function does not return anything but triggers the training process.\n",
        "      \"\"\"\n",
        "      start = time.time()\n",
        "      fold = fold\n",
        "      if self.model_name == 'llama with peft':\n",
        "          self.load_model_mistral(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'gpt3':\n",
        "          self.load_gpt3_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'bart large':\n",
        "          self.load_bart_large_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 't5 large':\n",
        "          self.load_t5_large_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'roberta large':\n",
        "          self.load_roberta_large_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'deberta v3 large':\n",
        "          self.load_deberta_v3_large_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'bert':\n",
        "          self.load_model_bert_cpu(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'roberta':\n",
        "          self.load_roberta_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'distilbert':\n",
        "          self.load_distilbert_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'albert':\n",
        "          self.load_albert_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'gpt2':\n",
        "          self.load_gpt2_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'xlnet':\n",
        "          self.load_xlnet_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'electra':\n",
        "          self.load_electra_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 't5 small':\n",
        "          self.load_t5_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'deberta':\n",
        "          self.load_deberta_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'mobilebert':\n",
        "          self.load_mobilebert_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'bart':\n",
        "          self.load_bart_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'convbert':\n",
        "          self.load_convbert_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      elif self.model_name == 'funnel':\n",
        "          self.load_funnel_model(fold)\n",
        "          self.train_model_by_fold(fold,self.model,self.tokenizer)\n",
        "      else:\n",
        "          raise ValueError(\"Model not recognized or not supported.\")\n",
        "    #     # Add multiple threads to run each fold model concurrently\n",
        "\n",
        "    #with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    #     futures = [executor.submit(train_model_by_fold, fold) for fold in range(2)]\n",
        "    #     # wait for all tasks to complete\n",
        "    #    wait(futures)\n",
        "    #    print('All training tasks are done!')\n",
        "\n",
        "    #for idx, fold in enumerate(range(N_FOLD)):\n",
        "    #sys.exit(f\"Training time of fold {fold} = {time.time() - start: .1f} seconds\")"
      ],
      "metadata": {
        "id": "uw7JExiMBmLp"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    \"\"\"\n",
        "    Seeds the random number generators of Python's `random`, NumPy, and PyTorch to ensure reproducibility.\n",
        "\n",
        "    Parameters:\n",
        "    - seed (int): A seed value to be used for all random number generators.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function does not return anything but sets the seed for various libraries.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "seed_everything()\n",
        "# Create new `pandas` methods which use `tqdm` progress\n",
        "# (can use tqdm_gui, optional kwargs, etc.)\n",
        "tqdm.pandas()\n",
        "\n",
        "log_level = \"DEBUG\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    level=logging.WARNING\n",
        ")\n",
        "\n",
        "# set the main code and the modules it uses to the same log-level according to the node\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "print(\"\"\"Available Models:\n",
        "llama with peft\n",
        "gpt3\n",
        "bart large\n",
        "t5 large\n",
        "roberta large\n",
        "deberta v3 large\n",
        "bert\n",
        "roberta\n",
        "distilbert\n",
        "albert\n",
        "gpt2\n",
        "xlnet\n",
        "electra\n",
        "t5 small\n",
        "deberta\n",
        "mobilebert\n",
        "bart\n",
        "convbert\n",
        "funnel\"\"\")\n",
        "\n",
        "model_name = input(\"Enter the desired model: \")\n",
        "# Cross validation with 5 fold\n",
        "train_data = load_training_data().training_data_setting_trigger()\n",
        "# Train the model\n",
        "fold = 0\n",
        "model_trainer(model_name='gpt2', train_data=train_data, DEBUG=True).language_model_training_trigger(fold)"
      ],
      "metadata": {
        "id": "MLfUGEZribqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "889c8318fa1d4a18bceec6c13563d141",
            "edb87b75f3a8426b8d1b5a8b116547dd",
            "8144987dbb8645ad97dc609e73e6c61a",
            "8cb17dd7260e4ab4b3c44934ebe45d55",
            "d3e5ab11b9e24dee964d3a2006090644",
            "66b2e2593d994925b902ab0c699ecc3b",
            "56cd0d8913e54ccfa86d09aa3ec7ed7d",
            "ad0dd6e4247440808de58bc4d93dc5dc",
            "4b1645e37de64b8587f4f8ed37f380fa",
            "d76f9e97d9ea4aa78ee791bb4fd25b57",
            "9fb750391c0e413f854622b3deb078fe"
          ]
        },
        "outputId": "f0d16e2a-3334-4b79-aa5b-8c13647734ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Models: \n",
            "llama with peft\n",
            "gpt3\n",
            "bart large\n",
            "t5 large\n",
            "roberta large\n",
            "deberta v3 large\n",
            "bert\n",
            "roberta\n",
            "distilbert\n",
            "albert\n",
            "gpt2\n",
            "xlnet\n",
            "electra\n",
            "t5 small\n",
            "deberta\n",
            "mobilebert\n",
            "bart\n",
            "convbert\n",
            "funnel\n",
            "Enter the desired model: gpt2\n",
            "Start processing external data's texts\n",
            "Train data label\n",
            "1    45068\n",
            "0    25956\n",
            "dtype: int64\n",
            "fold  label\n",
            "0.0   1        9014\n",
            "      0        5191\n",
            "1.0   1        9014\n",
            "      0        5191\n",
            "2.0   1        9014\n",
            "      0        5191\n",
            "3.0   1        9013\n",
            "      0        5192\n",
            "4.0   1        9013\n",
            "      0        5191\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text  label  fold\n",
              "0  Title: The Benefits of Limiting Car Usage: Ins...      1   3.0\n",
              "1  Title: The Benefits of Limiting Car Usage\\n\\nI...      1   3.0\n",
              "2  Title: The Advantages of Curtailing Car Usage:...      1   3.0\n",
              "3  Title: The Benefits of Limiting Car Usage\\n\\nH...      1   4.0\n",
              "4  Title: Advantages of Limiting Car Usage: A Ste...      1   4.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-360b0370-ba4c-432b-b5df-63f4ea9e8c9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Title: The Benefits of Limiting Car Usage: Ins...</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Title: The Benefits of Limiting Car Usage\\n\\nI...</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Title: The Advantages of Curtailing Car Usage:...</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Title: The Benefits of Limiting Car Usage\\n\\nH...</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Title: Advantages of Limiting Car Usage: A Ste...</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-360b0370-ba4c-432b-b5df-63f4ea9e8c9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-360b0370-ba4c-432b-b5df-63f4ea9e8c9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-360b0370-ba4c-432b-b5df-63f4ea9e8c9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1def1f87-da57-4e3d-a670-0766b8b0ed7f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1def1f87-da57-4e3d-a670-0766b8b0ed7f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1def1f87-da57-4e3d-a670-0766b8b0ed7f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/pytorch_model.bin\n",
            "Instantiating BartForSequenceClassification model under default dtype torch.float16.\n",
            "Detected 8-bit loading: activating 8-bit loading for this model\n",
            "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
            "\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\n",
            "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
            "\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the fold 0 model\n",
            "fold_train_df fold  label\n",
            "1.0   1        468\n",
            "      0        264\n",
            "2.0   1        453\n",
            "      0        257\n",
            "3.0   1        455\n",
            "      0        233\n",
            "4.0   1        433\n",
            "      0        278\n",
            "Name: label, dtype: int64\n",
            "fold_valid_df fold  label\n",
            "0.0   1        438\n",
            "      0        272\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2841 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "889c8318fa1d4a18bceec6c13563d141"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Dg1bJPiYTJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}